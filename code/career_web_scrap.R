#################################################
### step 0 : load packages, set wd ##########
####################################
setwd("C:/Users/traveler/Desktop/job_posting_recommendation/code")
currentwd <- getwd()

if(!require(tidyverse)){install.packages("tidyverse"); require(tidyverse)}
if(!require(rvest)){install.packages("rvest"); require(rvest)}
if(!require(RCurl)){install.packages("RCurl"); require(RCurl)}
if(!require(plyr)){install.packages("plyr"); require(plyr)}
if(!require(wrapr)){install.packages("wrapr"); require(wrapr)}
if(!require(sqldf)){install.packages("sqldf"); require(sqldf)}
if(!require(Hmisc)){install.packages("Hmisc"); require(Hmisc)}

###########################################
### step 1 : get links ############
###############################

# identify the url(s) with the necessary parameters, such as job title, salary, distance, etc.
da <- "https://www.glassdoor.com/Job/jobs.htm?sc.keyword=data%20analyst&locT=C&locId=1132348&locKeyword=New%20York,%20NY&jobType=fulltime&fromAge=7&minSalary=100000&includeNoSalaryJobs=true&radius=-1&cityId=-1&minRating=0.0&industryId=-1&companyId=-1&applicationType=0&employerSizes=0&remoteWorkType=0"

bi <- "https://www.glassdoor.com/Job/jobs.htm?sc.keyword=business%20intelligence&locT=C&locId=1132348&locKeyword=New%20York,%20NY&jobType=fulltime&fromAge=7&minSalary=100000&includeNoSalaryJobs=true&radius=-1&cityId=-1&minRating=0.0&industryId=-1&companyId=-1&applicationType=0&employerSizes=0&remoteWorkType=0"

# get the job links and jobListingId from the above urls
da_links <- read_html(da) %>%
        # html_nodes(".compactStars , .small , .empLoc div , .jobLink") %>%
        html_nodes(".jobLink") %>%
        html_attr("href") %>%
        na.omit %>%
        as.data.frame %>%
        dplyr::rename("links" = ".") %>%
        dplyr::mutate( links = paste0("https://www.glassdoor.com", as.character(links)),
                jobListingId = stringr::str_extract_all(links, pattern = "jobListingId=[0-9]+") %.>% 
                        gsub(pattern = "jobListingId=", replacement = "", x = .) ) %>%
        arrange(., jobListingId, desc(links)) %>%
        # many of these contain the same identical job posting but with different urls, we need to take them out
        group_by(jobListingId) %>%
        dplyr::mutate( id = row_number() ) %>%
        ungroup %>%
        dplyr::filter(id == 1) %>%
        select(links, jobListingId)

bi_links <- read_html(bi) %>%
        # html_nodes(".compactStars , .small , .empLoc div , .jobLink") %>%
        html_nodes(".jobLink") %>%
        html_attr("href") %>%
        na.omit %>%
        as.data.frame %>%
        dplyr::rename("links" = ".") %>%
        dplyr::mutate( links = paste0("https://www.glassdoor.com", as.character(links)),
                       jobListingId = stringr::str_extract_all(links, pattern = "jobListingId=[0-9]+") %.>% 
                               gsub(pattern = "jobListingId=", replacement = "", x = .) ) %>%
        arrange(., jobListingId, desc(links)) %>%
        # many of these contain the same identical job posting but with different urls, we need to take them out
        group_by(jobListingId) %>%
        dplyr::mutate( id = row_number() ) %>%
        ungroup %>%
        dplyr::filter(id == 1) %>%
        select(links, jobListingId)

# append the df
links <- rbind(da_links, bi_links) %>% distinct

# take a look of these links
# links %>%
#         arrange(desc(jobListingId)) %.>%
#         write.table(., "clipboard", row.names = F)

###################################################
### step 2 : get job info for each link #######
###########################################

# create extract function 
extract_job_info <- function(link, jobListingId) {
        read_html(link) %>% 
        html_nodes(".nowrap, .ratingNum , .salEst , .desc , #HeroHeaderModule .strong") %>%
        html_text %>%
        as.data.frame %>%
        dplyr::rename("job" = ".") %>%
        t %>%
        as.data.frame %>% 
        dplyr::mutate(jobListingId = jobListingId)        
}

# must use purrr::possibly - fail-safe approach
extract_job_info <- purrr::possibly(extract_job_info, otherwise = NA_real_)

# create an empty list
jobList <- vector(mode = "list", length = nrow(links))

# scrape the job info and write each into the list before complying into a data.frame 
system.time( lapply(1:length(jobList), function(x){
        jobList[[x]] <<- extract_job_info(links$links[x], links$jobListingId[x])
        } ) %.>% invisible(.) )

# remove NA result first
na <- lapply(1:length(jobList), function(x) is.na(jobList[x])) %>% unlist %>% which
stay <- c(1:length(jobList))[1:length(jobList) %nin% na]

jobList <- jobList[stay]

# there's missing data in different job postings
# some have 4 or 5 columns, while most have 8 (complete set)
jobListSummary <- data.frame(row = 1:length(jobList), length = NA)

lapply(1:length(jobList), function(x) {
        jobListSummary$length[x] <<- length(jobList[[x]])
}) %.>% invisible(.)

uniqueLength <- unique(jobListSummary$length)
lengthUniqueLength <- length(uniqueLength)
uniqueList <- vector(mode = "list", length = lengthUniqueLength)

for(i in 1:lengthUniqueLength){
        uniqueList[[i]] <- sqldf::sqldf(
                sprintf('select row from jobListSummary where length = %d', uniqueLength[i]) 
        )
}

uniqueList <- lapply(1:length(uniqueList), function(x){
        unlist(uniqueList[[x]]) %>% as.vector
})

# store each set of data into a data.frame by the length of column
jobListDf <- vector(mode = "list", length = lengthUniqueLength)

for(i in 1:lengthUniqueLength){
        jobListDf[[i]] <- jobList[c(uniqueList[[i]])]
}

# jobListDf is now a list of lists (by length of column) in which each list contains a data.frame for each job
# str(jobListDf); summary(jobListDf)
# let's reduce it to a number of list by length of column and make each list a data.frame (combining all jobs inside that list)
putTogether <- function(x){
        plyr::ldply(x, data.frame)
}

jobListDf <- lapply(jobListDf, putTogether)
# str(jobListDf)

#########################################################################
### step 3 : clean up each column, inner join back with "links" #######
###############################################################

### CAUTION : there are multiple lists - some have 9 columns, whereas some have only 5 or 6 ###
# let's just look at the complete set (9 columns)
# because the other are missing critical information that we need, e.g. review, salary

extractListNumber <- lapply(1:length(jobListDf), function(x){
        length(jobListDf[[x]]) == 9}) %>% 
        unlist %.>%
        which(. == T)

jobDf <- jobListDf[[extractListNumber]] %>%
        dplyr::mutate( title = as.character(V1),
                       company = as.character(V2) %>% 
                               stringr::str_trim(.),
                       last_updated = stringr::str_extract_all(V3, pattern = "Today|[0-9]+") %>% 
                               unlist %.>% 
                               ifelse(. == "Today", 0, .) %>%
                               as.numeric %.>%
                               lapply(1:length(.), function(x){
                                       Sys.Date() -.[x]
                               }) %>% 
                               unlist %.>%
                               as.Date(., origin = "1970-01-01"), 
                       glassdoor_est = as.character(V4),
                       review = stringr::str_extract_all(V6, pattern = "[0-9]\\.[0-9]") %>% 
                               as.numeric,
                       salary_est = gsub(pattern = ",", replacement = "", V7) %>%
                               stringr::str_extract_all(., pattern = "[0-9]+") %>%
                               unlist %>%
                               as.numeric,
                       salary_est = salary_est / 1000,
                       jobListingId = as.character(jobListingId),
                       description = as.character(V8),
                       run_time = Sys.time() ) %>%
        tidyr::separate(., glassdoor_est, into = c("salary_min_est", "salary_max_est"), sep = "-") %>%
        mutate( salary_min_est = stringr::str_extract_all(salary_min_est, pattern = "[0-9]+") %>%
                        unlist %>%
                        as.numeric,
                salary_max_est = stringr::str_extract_all(salary_max_est, pattern = "[0-9]+") %>%
                        unlist %>%
                        as.numeric ) %>%
        # inner join it back with "links" to get url pairing with jobListingId
        dplyr::inner_join(., links, by = "jobListingId") %>%
        dplyr::rename(url = links) %>%
        select(jobListingId, title, company, last_updated, 
               salary_min_est, salary_max_est, salary_est, 
               review, url, run_time, description) %>%
        arrange(company, title, last_updated)

# setwd to data/job
setwd("../"); setwd("data/job")

# save as txt
run_time <- jobDf$run_time %>% unique %.>% gsub(pattern = ":", replacement = "", x = .)
filename <- paste0("jobDf_", run_time, ".txt", sep = "")
write.table(jobDf, file = filename, sep = "\t", row.names = F, append = F)

# reset wd
setwd(currentwd)

###################################################
### step 4 : get, add company overview #######
###########################################

# write a function to search via google for the url pointing to the glassdoor overview database for a company
# wish I can just get an API to query against glassdoor/Indeed database
overview <- function(company){
        entity = company
        search = "glassdoor company overview"
        url = URLencode(paste0("https://www.google.com/search?q=", paste0(entity, search, sep = " ")))
        
        # return top pages result
        cite <- read_html(url) %>% 
                html_nodes("cite") %>%  # change any node you like, e.g. cite
                html_text() %>%
                as.data.frame(., stringsAsFactors = F) %>%
                dplyr::mutate(row = row_number()) %>%
                dplyr::rename(url = ".")
        
        # sql hack - return only the "lower(url) like '%glassdoor%overview/working-at%'"
        overview_cite_select <- sqldf("select url 
                                from (
                                select url, min(row) as flag 
                                from ( select url, row 
                                        from cite 
                                        where lower(url) like '%glassdoor%overview/working-at%' ) x 
                                group by url
                                ) y") %>% as.character
        
        # scrape the company overview
        overview <- read_html(overview_cite_select) %>%
                html_nodes(".value , .website") %>%
                html_text %>%
                # dplyr::rename("overview" = ".") %>%
                t %>%
                as.data.frame %>%
                dplyr::mutate(V0 = company) %>%
                select(., starts_with("V"))
        
        return(overview)
}

# must use purrr::possibly - fail-safe approach
overview <- purrr::possibly(overview, otherwise = NA_real_)

# start scraping from the list of companies from jobDf
companies <- vector(mode = "list", length = length(unique(jobDf$company)))
names(companies) <- jobDf$company %>% unique 
companies <- lapply(1:length(companies), function(x) companies[[x]] <- names(companies)[x])
# system.time( companies <- lapply(1:length(companies), function(x) overview(companies[[x]])) )
system.time( companies <- purrr::map(1:length(companies), function(x) overview(companies[[x]])) )  # a little faster

# remove NA result first
remove <- lapply(1:length(companies), function(x) is.na(companies[x])) %>% unlist %>% which
keep <- c(1:length(companies))[1:length(companies) %nin% remove]

# extract only V0:V7 columns
extract <- function(x){
        x <- dplyr::select(x, V0, V1, V2, V3, V4, V5, V6, V7)
        return(x)
}

# must use purrr::safely - fail-safe approach
extract_safely <- purrr::safely(extract, otherwise = NA_real_)

# extract, combine into a single df, and then rename columns
companyDf <- purrr::map(companies[keep], extract_safely) %.>%
        purrr::transpose(.)

companyDf <- companyDf$result %>%
        plyr::ldply(., data.frame)

names(companyDf) <- c("company", "website", "headquarters", "size",
                      "founded", "type", "industry", "revenue")

# change Factor to Chr, use str_trim to remove white space
companyDf <- companyDf[, 1:8]  # have to subset first; dplyr::select does not allow NA column
companyDf <- companyDf[complete.cases(companyDf), ] %>%
        dplyr::mutate( website = as.character(website) %>% stringr::str_trim(.),
                       headquarters = as.character(headquarters) %>% stringr::str_trim(.),
                       size = as.character(size) %>% stringr::str_trim(.),
                       founded = stringr::str_extract_all(founded, "[0-9]+") %>% as.numeric,
                       type = as.character(type) %.>%
                               gsub(".*[Pp]rivate.*", "private", .) %.>%
                               gsub(".*[Pp]ublic.*", "public", .) %>%
                               stringr::str_trim(.),
                       industry = as.character(industry) %>% stringr::str_trim(.),
                       revenue = as.character(revenue) %>% stringr::str_trim(.),
                       run_time = Sys.time() )

# complete.cases() again, b/c of some misplacing columns
# some jobs have 10 columns and there's some mismatch when we manually pull V1:V7 columns
# for example, "founded" value would be misplaced in the "type" column
# an imperfect and temporary solution is to do complete.cases() one more time to rid the NA
companyDf <- companyDf[complete.cases(companyDf), ]

# setwd to data/company
setwd("../"); setwd("data/company")

# save as txt
run_time2 <- companyDf$run_time %>% unique %.>% gsub(pattern = ":", replacement = "", x = .)
filename2 <- paste0("companyDf_", run_time2, ".txt", sep = "")
write.table(companyDf, file = filename2, sep = "\t", row.names = F, append = F)

# reset wd
setwd(currentwd)

##############################################
##### step 5.1 : update (delete), insert #####
##############################################

setwd("../"); setwd("data")

# load the _agg tables
jobDf_agg <- readRDS(file = "jobDf_agg.RDS")
companyDf_agg <- readRDS(file = "companyDf_agg.RDS")

#######################
# delete, insert, save
# job
jobListingId_agg <- jobDf_agg$jobListingId   
jobListingId_now <- jobDf$jobListingId

del <- jobListingId_agg[jobListingId_agg %in% jobListingId_now]

jobDf_agg <- jobDf_agg %>%
        dplyr::filter(jobListingId %nin% del) %.>%  # delete
        dplyr::bind_rows(., jobDf)  # append

saveRDS(jobDf_agg, file = "jobDf_agg.RDS")  # save

# company
company_agg <- companyDf_agg$company   
company_now <- companyDf$company

del2 <- company_agg[company_agg %in% company_now]

companyDf_agg <- companyDf_agg %>%
        dplyr::filter(company %nin% del2) %.>%  # delete
        dplyr::bind_rows(., companyDf)  # append

saveRDS(companyDf_agg, file = "companyDf_agg.RDS")  # save
#######################

# reset wd
setwd(currentwd)

############################
##### step 5.2 : merge #####
############################

### CAUTION : there is always missing data for some companies overview ###
# let's merge (instead of left join) jobDf_agg and companyDf_agg together

jpr <- merge(jobDf_agg, companyDf_agg, by = "company") %>%
        # filter job titles that are not relevant
        dplyr::filter(title %in% title[!grepl("(architect|software|engineer|director)", title, ignore.case = T)]) %>%
        # filter by job posting last_updated within past 14 days
        # dplyr::filter(last_updated > Sys.Date() -14) %>%
        # filter out duplicated jobListingId - this happens b/c same job with same Id can have different urls
        group_by(jobListingId) %>%
        dplyr::mutate(row = row_number()) %>%
        ungroup %>%
        dplyr::filter(row == 1) %>%
        select(jobListingId, title, company, last_updated, salary_min_est, salary_max_est, salary_est, review, url,
               website, headquarters, size, founded, type, industry, revenue, description) %>%
        arrange(company, title, last_updated) 

View(jpr)







