setwd("C:/Users/traveler/Desktop/job_posting_recommendation/code")
currentwd <- getwd()
if(!require(tidyverse)){install.packages("tidyverse"); require(tidyverse)}
if(!require(rvest)){install.packages("rvest"); require(rvest)}
if(!require(RCurl)){install.packages("RCurl"); require(RCurl)}
if(!require(plyr)){install.packages("plyr"); require(plyr)}
if(!require(wrapr)){install.packages("wrapr"); require(wrapr)}
if(!require(sqldf)){install.packages("sqldf"); require(sqldf)}
if(!require(Hmisc)){install.packages("Hmisc"); require(Hmisc)}
if(!require(tidytext)){install.packages("tidytext"); require(tidytext)}
if(!require(tm)){install.packages("tm"); require(tm)}
if(!require(e1071)){install.packages("e1071"); require(e1071)}
if(!require(pROC)){install.packages("pROC"); require(pROC)}
if(!require(wordcloud)){install.packages("wordcloud"); require(wordcloud)}
###########################################
### step 1 : get links ############
###############################
# identify the url(s) with the necessary parameters, such as job title, salary, distance, etc.
da <- "https://www.glassdoor.com/Job/jobs.htm?sc.keyword=data%20analyst&locT=C&locId=1132348&locKeyword=New%20York,%20NY&jobType=fulltime&fromAge=7&minSalary=100000&includeNoSalaryJobs=true&radius=-1&cityId=-1&minRating=0.0&industryId=-1&companyId=-1&applicationType=0&employerSizes=0&remoteWorkType=0"
bi <- "https://www.glassdoor.com/Job/jobs.htm?sc.keyword=business%20intelligence&locT=C&locId=1132348&locKeyword=New%20York,%20NY&jobType=fulltime&fromAge=7&minSalary=100000&includeNoSalaryJobs=true&radius=-1&cityId=-1&minRating=0.0&industryId=-1&companyId=-1&applicationType=0&employerSizes=0&remoteWorkType=0"
# get the job links and jobListingId from the above urls
da_links <- read_html(da) %>%
# html_nodes(".compactStars , .small , .empLoc div , .jobLink") %>%
html_nodes(".jobLink") %>%
html_attr("href") %>%
na.omit %>%
as.data.frame %>%
dplyr::rename("links" = ".") %>%
dplyr::mutate( links = paste0("https://www.glassdoor.com", as.character(links)),
jobListingId = stringr::str_extract_all(links, pattern = "jobListingId=[0-9]+") %.>%
gsub(pattern = "jobListingId=", replacement = "", x = .) ) %>%
arrange(., jobListingId, desc(links)) %>%
# many of these contain the same identical job posting but with different urls, we need to take them out
group_by(jobListingId) %>%
dplyr::mutate( id = row_number() ) %>%
ungroup %>%
dplyr::filter(id == 1) %>%
select(links, jobListingId)
bi_links <- read_html(bi) %>%
# html_nodes(".compactStars , .small , .empLoc div , .jobLink") %>%
html_nodes(".jobLink") %>%
html_attr("href") %>%
na.omit %>%
as.data.frame %>%
dplyr::rename("links" = ".") %>%
dplyr::mutate( links = paste0("https://www.glassdoor.com", as.character(links)),
jobListingId = stringr::str_extract_all(links, pattern = "jobListingId=[0-9]+") %.>%
gsub(pattern = "jobListingId=", replacement = "", x = .) ) %>%
arrange(., jobListingId, desc(links)) %>%
# many of these contain the same identical job posting but with different urls, we need to take them out
group_by(jobListingId) %>%
dplyr::mutate( id = row_number() ) %>%
ungroup %>%
dplyr::filter(id == 1) %>%
select(links, jobListingId)
# append the df
links <- rbind(da_links, bi_links) %>% distinct
# take a look of these links
# links %>%
#         arrange(desc(jobListingId)) %.>%
#         write.table(., "clipboard", row.names = F)
###################################################
### step 2 : get job info for each link #######
###########################################
# create extract function
extract_job_info <- function(link, jobListingId) {
read_html(link) %>%
html_nodes(".nowrap, .ratingNum , .salEst , .desc , #HeroHeaderModule .strong") %>%
html_text %>%
as.data.frame %>%
dplyr::rename("job" = ".") %>%
t %>%
as.data.frame %>%
dplyr::mutate(jobListingId = jobListingId)
}
# must use purrr::possibly - fail-safe approach
extract_job_info <- purrr::possibly(extract_job_info, otherwise = NA_real_)
# create an empty list
jobList <- vector(mode = "list", length = nrow(links))
# scrape the job info and write each into the list before complying into a data.frame
system.time( lapply(1:length(jobList), function(x){
jobList[[x]] <<- extract_job_info(links$links[x], links$jobListingId[x])
} ) %.>% invisible(.) )
# remove NA result first
na <- lapply(1:length(jobList), function(x) is.na(jobList[x])) %>% unlist %>% which
stay <- c(1:length(jobList))[1:length(jobList) %nin% na]
jobList <- jobList[stay]
# there's missing data in different job postings
# some have 4 or 5 columns, while most have 8 (complete set)
jobListSummary <- data.frame(row = 1:length(jobList), length = NA)
lapply(1:length(jobList), function(x) {
jobListSummary$length[x] <<- length(jobList[[x]])
}) %.>% invisible(.)
uniqueLength <- unique(jobListSummary$length)
lengthUniqueLength <- length(uniqueLength)
uniqueList <- vector(mode = "list", length = lengthUniqueLength)
for(i in 1:lengthUniqueLength){
uniqueList[[i]] <- sqldf::sqldf(
sprintf('select row from jobListSummary where length = %d', uniqueLength[i])
)
}
uniqueList <- lapply(1:length(uniqueList), function(x){
unlist(uniqueList[[x]]) %>% as.vector
})
# store each set of data into a data.frame by the length of column
jobListDf <- vector(mode = "list", length = lengthUniqueLength)
for(i in 1:lengthUniqueLength){
jobListDf[[i]] <- jobList[c(uniqueList[[i]])]
}
# jobListDf is now a list of lists (by length of column) in which each list contains a data.frame for each job
# str(jobListDf); summary(jobListDf)
# let's reduce it to a number of list by length of column and make each list a data.frame (combining all jobs inside that list)
putTogether <- function(x){
plyr::ldply(x, data.frame)
}
jobListDf <- lapply(jobListDf, putTogether)
# str(jobListDf)
#########################################################################
### step 3 : clean up each column, inner join back with "links" #######
###############################################################
### CAUTION : there are multiple lists - some have 9 columns, whereas some have only 5 or 6 ###
# let's just look at the complete set (9 columns)
# because the other are missing critical information that we need, e.g. review, salary
extractListNumber <- lapply(1:length(jobListDf), function(x){
length(jobListDf[[x]]) == 9}) %>%
unlist %.>%
which(. == T)
jobDf <- jobListDf[[extractListNumber]] %>%
dplyr::mutate( title = as.character(V1),
company = as.character(V2) %>%
stringr::str_trim(.),
last_updated = stringr::str_extract_all(V3, pattern = "Today|[0-9]+") %>%
unlist %.>%
ifelse(. == "Today", 0, .) %>%
as.numeric %.>%
lapply(1:length(.), function(x){
Sys.Date() -.[x]
}) %>%
unlist %.>%
as.Date(., origin = "1970-01-01"),
glassdoor_est = as.character(V4),
review = stringr::str_extract_all(V6, pattern = "[0-9]\\.[0-9]") %>%
as.numeric,
salary_est = gsub(pattern = ",", replacement = "", V7) %>%
stringr::str_extract_all(., pattern = "[0-9]+") %>%
unlist %>%
as.numeric,
salary_est = salary_est / 1000,
jobListingId = as.character(jobListingId),
description = as.character(V8),
run_time = Sys.time() ) %>%
tidyr::separate(., glassdoor_est, into = c("salary_min_est", "salary_max_est"), sep = "-") %>%
mutate( salary_min_est = stringr::str_extract_all(salary_min_est, pattern = "[0-9]+") %>%
unlist %>%
as.numeric,
salary_max_est = stringr::str_extract_all(salary_max_est, pattern = "[0-9]+") %>%
unlist %>%
as.numeric ) %>%
# inner join it back with "links" to get url pairing with jobListingId
dplyr::inner_join(., links, by = "jobListingId") %>%
dplyr::rename(url = links) %>%
select(jobListingId, title, company, last_updated,
salary_min_est, salary_max_est, salary_est,
review, url, run_time, description) %>%
arrange(company, title, last_updated)
# setwd to data/job
setwd("../"); setwd("data/job")
# save as txt
run_time <- jobDf$run_time %>% unique %.>% gsub(pattern = ":", replacement = "", x = .)
filename <- paste0("jobDf_", run_time, ".txt", sep = "")
write.table(jobDf, file = filename, sep = "\t", row.names = F, append = F)
# reset wd
setwd(currentwd)
###################################################
### step 4 : get, add company overview #######
###########################################
# write a function to search via google for the url pointing to the glassdoor overview database for a company
# wish I can just get an API to query against glassdoor/Indeed database
overview <- function(company){
entity = company
search = "glassdoor company overview"
url = URLencode(paste0("https://www.google.com/search?q=", paste0(entity, search, sep = " ")))
# return top pages result
cite <- read_html(url) %>%
html_nodes("cite") %>%  # change any node you like, e.g. cite
html_text() %>%
as.data.frame(., stringsAsFactors = F) %>%
dplyr::mutate(row = row_number()) %>%
dplyr::rename(url = ".")
# sql hack - return only the "lower(url) like '%glassdoor%overview/working-at%'"
overview_cite_select <- sqldf("select url
from (
select url, min(row) as flag
from ( select url, row
from cite
where lower(url) like '%glassdoor%overview/working-at%' ) x
group by url
) y") %>% as.character
# scrape the company overview
overview <- read_html(overview_cite_select) %>%
html_nodes(".value , .website") %>%
html_text %>%
# dplyr::rename("overview" = ".") %>%
t %>%
as.data.frame %>%
dplyr::mutate(V0 = company) %>%
select(., starts_with("V"))
return(overview)
}
# must use purrr::possibly - fail-safe approach
overview <- purrr::possibly(overview, otherwise = NA_real_)
# start scraping from the list of companies from jobDf
companies <- vector(mode = "list", length = length(unique(jobDf$company)))
names(companies) <- jobDf$company %>% unique
companies <- lapply(1:length(companies), function(x) companies[[x]] <- names(companies)[x])
# system.time( companies <- lapply(1:length(companies), function(x) overview(companies[[x]])) )
system.time( companies <- purrr::map(1:length(companies), function(x) overview(companies[[x]])) )  # a little faster
# remove NA result first
remove <- lapply(1:length(companies), function(x) is.na(companies[x])) %>% unlist %>% which
keep <- c(1:length(companies))[1:length(companies) %nin% remove]
# extract only V0:V7 columns
extract <- function(x){
x <- dplyr::select(x, V0, V1, V2, V3, V4, V5, V6, V7)
return(x)
}
# must use purrr::safely - fail-safe approach
extract_safely <- purrr::safely(extract, otherwise = NA_real_)
# extract, combine into a single df, and then rename columns
companyDf <- purrr::map(companies[keep], extract_safely) %.>%
purrr::transpose(.)
companyDf <- companyDf$result %>%
plyr::ldply(., data.frame)
names(companyDf) <- c("company", "website", "headquarters", "size",
"founded", "type", "industry", "revenue")
# change Factor to Chr, use str_trim to remove white space
companyDf <- companyDf[, 1:8]  # have to subset first; dplyr::select does not allow NA column
companyDf <- companyDf[complete.cases(companyDf), ] %>%
dplyr::mutate( website = as.character(website) %>% stringr::str_trim(.),
headquarters = as.character(headquarters) %>% stringr::str_trim(.),
size = as.character(size) %>% stringr::str_trim(.),
founded = stringr::str_extract_all(founded, "[0-9]+") %>% as.numeric,
type = as.character(type) %.>%
gsub(".*[Pp]rivate.*", "private", .) %.>%
gsub(".*[Pp]ublic.*", "public", .) %>%
stringr::str_trim(.),
industry = as.character(industry) %>% stringr::str_trim(.),
revenue = as.character(revenue) %>% stringr::str_trim(.),
run_time = Sys.time() )
# complete.cases() again, b/c of some misplacing columns
# some jobs have 10 columns and there's some mismatch when we manually pull V1:V7 columns
# for example, "founded" value would be misplaced in the "type" column
# an imperfect and temporary solution is to do complete.cases() one more time to rid the NA
companyDf <- companyDf[complete.cases(companyDf), ]
# setwd to data/company
setwd("../"); setwd("data/company")
# save as txt
run_time2 <- companyDf$run_time %>% unique %.>% gsub(pattern = ":", replacement = "", x = .)
filename2 <- paste0("companyDf_", run_time2, ".txt", sep = "")
write.table(companyDf, file = filename2, sep = "\t", row.names = F, append = F)
# reset wd
setwd(currentwd)
##############################################
##### step 5.1 : update (delete), insert #####
##############################################
setwd("../"); setwd("data")
# load the _agg tables
jobDf_agg <- readRDS(file = "jobDf_agg.RDS")
companyDf_agg <- readRDS(file = "companyDf_agg.RDS")
#######################
# delete, insert, save
# job
jobListingId_agg <- jobDf_agg$jobListingId
jobListingId_now <- jobDf$jobListingId
del <- jobListingId_agg[jobListingId_agg %in% jobListingId_now]
jobDf_agg <- jobDf_agg %>%
dplyr::filter(jobListingId %nin% del) %.>%  # delete
dplyr::bind_rows(., jobDf)  # append
saveRDS(jobDf_agg, file = "jobDf_agg.RDS")  # save
# company
company_agg <- companyDf_agg$company
company_now <- companyDf$company
del2 <- company_agg[company_agg %in% company_now]
companyDf_agg <- companyDf_agg %>%
dplyr::filter(company %nin% del2) %.>%  # delete
dplyr::bind_rows(., companyDf)  # append
saveRDS(companyDf_agg, file = "companyDf_agg.RDS")  # save
#######################
# reset wd
setwd(currentwd)
############################
##### step 5.2 : merge #####
############################
### CAUTION : there is always missing data for some companies overview ###
# let's merge (instead of left join) jobDf_agg and companyDf_agg together
jpr <- merge(jobDf_agg, companyDf_agg, by = "company") %>%
# filter job titles that are not relevant
dplyr::filter(title %in% title[!grepl("(architect|software|engineer|director)", title, ignore.case = T)]) %>%
# filter by job posting last_updated within past 14 days
# dplyr::filter(last_updated > Sys.Date() -14) %>%
# filter out duplicated jobListingId - this happens b/c same job with same Id can have different urls
group_by(jobListingId) %>%
dplyr::mutate(row = row_number()) %>%
ungroup %>%
dplyr::filter(row == 1) %>%
select(jobListingId, title, company, last_updated, salary_min_est, salary_max_est, salary_est, review, url,
website, headquarters, size, founded, type, industry, revenue, description) %>%
arrange(company, title, last_updated)
jpr %>% arrange(., desc(last_updated)) %>% View
# output into txt, for manually review
# write.table(jpr, file = "output.txt", row.names = F)
#
# ####################################################################################
# ################################# ANALYSIS BEGINS! #################################
# ########### step 6 : review jobs, flag out those that I am interested in ###########
# ####################################################################################
#
# # manully go through the short list (200 jobs) - flag out those that I am interested in
# # merge it back and create a new data frame (newDf) for modeling
# # flag_job_id <- readit("flag_job_id.xlsx")  # has only two columns, i.e. jobListingId, flag
# # newDf <- merge(flag_job_id, jpr) %>%
# #         # select columns that I need for modeling
# #         dplyr::select(jobListingId, flag, salary_est, review,
# #                       size, industry, revenue, description) %>%
# #         dplyr::mutate(size = as.factor(size),
# #                       industry = as.factor(industry),
# #                       revenue = as.factor(revenue))
#
# # setwd("../"); setwd("data")
# # newDf <- read.table("newDf.csv", header = T, stringsAsFactors = T)
# newDf <- read.table("https://raw.githubusercontent.com/myvioletrose/job_posting_recommendation/master/data/newDf.csv", header = T, stringsAsFactors = T)
# newDf <- newDf %>% dplyr::mutate(description = as.character(description))
# str(newDf)
#
# # setwd(currentwd)
#
# # create description data frame for text mining
# description <- newDf %>% dplyr::select(jobListingId, flag, description)
# table(description$flag)  # I am interested in 60 out of 200 jobs scrapped from glassdoor
#
# ### the goal is to combine quantitative variables (like estimated salary, company revenue) with text description of a job to predict my potential interest in a job!
#
#
# ######################################
# ######## step 7 : text mining ########
# ######################################
#
# ### cleanup steps ###
#
# # first put the corpus in tm format
# descriptionClean <- Corpus(VectorSource(description$description))
#
# # clean up
# descriptionClean <- tm_map(descriptionClean, content_transformer(tolower))
# descriptionClean <- tm_map(descriptionClean, removeWords, stopwords())
# descriptionClean <- tm_map(descriptionClean, stripWhitespace)
# descriptionClean <- tm_map(descriptionClean, removePunctuation)
#
# # convert it into a dtm (row per document, column per word)
# dtm <- DocumentTermMatrix(descriptionClean)
# # inspect(dtm)
#
# # set frequency filter, i.e. only include words that appear f or more times in the whole corpus
# f = 10
# features <- findFreqTerms(dtm, f)
#
# #####################################################
# ##### step 8 : split into train, test data sets #####
# #####################################################
#
# # set index : split by 70% vs 30%
# set.seed(1234)
# index <- sample(1:dim(description)[1], .7 * dim(description)[1])
#
# # step 1 : split original corpus into train and test sets, each set contains the "flag" (dependent variable)
# train_step_1 <- description[index, ]
# test_step_1 <- description[-index, ]
#
# # step 2 : dummify the "term" (or word) columns
# train_step_2 <- descriptionClean[index] %>%
#         DocumentTermMatrix(., list(global = c(2, Inf), dictionary = features)) %>%
#         apply(MARGIN = 2, function(x) x <- ifelse(x >0, 1, 0)) %>%
#         as.data.frame
#
# test_step_2 <- descriptionClean[-index] %>%
#         DocumentTermMatrix(., list(global = c(2, Inf), dictionary = features)) %>%
#         apply(MARGIN = 2, function(x) x <- ifelse(x >0, 1, 0)) %>%
#         as.data.frame
#
# # step 3 : put step 1 and 2 together
# train <- cbind(flag = factor(train_step_1$flag),
#                jobListingId = train_step_1$jobListingId,
#                train_step_2) %>% as.data.frame
#
# test <- cbind(flag = factor(test_step_1$flag),
#               jobListingId = test_step_1$jobListingId,
#               test_step_2) %>% as.data.frame
#
# # FINAL step : merge back with newDf
# # minus "industry" because we don't collect enough data
# # the train data set is missing some industries in the test data set
# newDf_train_subset <- newDf[index, ] %>% select(-c(description, flag, industry))
# newDf_test_subset <- newDf[-index, ] %>% select(-c(description, flag, industry))
#
# train <- merge(train, newDf_train_subset, by = "jobListingId") %>% dplyr::select(-jobListingId)
# test <- merge(test, newDf_test_subset, by = "jobListingId") %>% dplyr::select(-jobListingId)
#
#
# ##############################################################
# ############### step 9 : classification models ###############
# ##############################################################
#
# #######################
# # Logistic Regression #
# #######################
#
# # build a model
# fit_lr <- glm(flag ~., train, family = "binomial")  # summary(fit_lr)
#
# # fit a prediction
# fit_lr_pred <- predict(fit_lr, newdata = test[, -1], type = "response")
#
# # classification outcome
# ftable(test$flag, fit_lr_pred > 0.5) -> table_lr
# table_lr
#
# table_lr %>% prop.table(., margin = 1)*100 -> accuracy_lr
# round(accuracy_lr, 1)
#
# # ROC curve
# fit_lr_pred_roc <- roc(flag ~ fit_lr_pred, data = test)
# plot(fit_lr_pred_roc, main = "ROC curve of logistic regression model")
#
#
# ####################################################
# ############### step 10 : Word Cloud ###############
# ####################################################
#
# ### let's visualize solely jobs that I am interested in by using word cloud
#
# ####### overall word cloud #######
#
# # clean text first
# clean.text = function(x)
# {
#         # tolower
#         x = tolower(x)
#         # remove rt
#         x = gsub("rt", "", x)
#         # remove at
#         x = gsub("@\\w+", "", x)
#         # remove punctuation
#         x = gsub("[[:punct:]]", "", x)
#         # remove numbers
#         x = gsub("[[:digit:]]", "", x)
#         # remove links http
#         x = gsub("http\\w+", "", x)
#         # remove tabs
#         x = gsub("[ |\t]{2,}", "", x)
#         # remove blank spaces at the beginning
#         x = gsub("^ ", "", x)
#         # remove blank spaces at the end
#         x = gsub(" $", "", x)
#         return(x)
# }
#
# overall <- tm::Corpus(VectorSource(description$description[description$flag == 1])) %>%
#         clean.text
#
# set.seed(1234)
# wordcloud(overall,
#           min.freq = 30,
#           colors = brewer.pal(8, "RdBu"),
#           scale = c(9, .7))
# # savePlot(filename = "word_cloud.png", type = "png")
#
# ####### comparison cloud #######
#
# # clean, transform into tdm first
# interested <- description %>% filter(flag == 1) %>% select(description) %>%
#         clean.text %>%
#         paste(., collapse = " ")
#
# not_interested <- description %>% filter(flag == 0) %>% select(description) %>%
#         clean.text %>%
#         paste(., collapse = " ")
#
# all <- c(interested, not_interested) %>%
#         removeWords(., c(stopwords("english"))) %>%
#         VectorSource %>%
#         Corpus
#
# tdm <- TermDocumentMatrix(all) %>% as.matrix
# colnames(tdm) <- c("Interested", "Not Interested")
#
# # comparison cloud #
# set.seed(1234)
# comparison.cloud(tdm,
#                  title.size = 1,
#                  random.order = FALSE,
#                  # colors = c("#00B2FF", "red", "#FF0099", "#6600CC"),
#                  colors = c("#00B2FF", "#6600CC"),
#                  max.words = 200,
#                  scale = c(8, .2))
#
# # savePlot(filename = "comparison_cloud.png", type = "png")
#
# # save final df output
# # write.table(newDf, "newDf.csv", row.names = F, append = F)
# # df <- read.table("newDf.csv", header = T)
#
ls()
View(links)
