data(breslow.dat, package = "robust")
ls()
head(breslow.dat)
attach(breslow.dat)
fit <- glm(SumY ~ Base + Age + Trt, data = breslow.dat, family = poisson())
summary(fit)
fit <- glm(sumY ~ Base + Age + Trt, data = breslow.dat, family = poisson())
summary(fit)
?predict
?base::predict
summary(breslow.dat)
summary(breslow.dat)
head(breslow.dat)
ob <- data.frame(Base = c(22, 41), Age = c(30, 40), Trt = c("placebo", "progabide"))
predict(fit, ob)
ob <- data.frame(Base = c(20, 20), Age = c(30, 40), Trt = c("placebo", "progabide"))
predict(fit, ob)
exp(coef(fit))
1.023 ^ 10
ob <- data.frame(Base = c(20, 20), Age = c(30, 40), Trt = c("placebo", "placebo"))
predict(fit, ob)
1.0229102 ^ 20 + 1.0230007 ^ 40 + 0.8583864
(1.0229102 ^ 20) * (1.0230007 ^ 40) * 0.8583864
(1.0229102 ^ 20) * (1.0230007 ^ 40) * 1 / 0.8583864
(1.0229102 ^ 20) * (1.0230007 ^ 40) * 1
ob <- data.frame(Base = c(20, 20), Age = c(30, 40), Trt = c("progabide", "progabide"))
predict(fit, ob)
(1.0229102 ^ 20) * (1.0230007 ^ 40) * 0.8583864
(1.0229102 ^ 20) + (1.0230007 ^ 40) + 0.8583864
ob <- data.frame(Base = c(20, 20), Age = c(30, 40), Trt = c("progabide", "progabide"))
predict(fit, ob)
exp(coef(fit))
7.0204403 * (1.0229102 ^ 20) * (1.0230007 ^ 40) * 0.8583864
exp(predict(fit, ob))
exp(0.8583864)
coef(fit)
exp(1.94882593 * (0.02265174 ^ 20) * (0.02274013 ^ 40) * -0.15270095)
1.94882593 * (0.02265174 ^ 20) * (0.02274013 ^ 40) * -0.15270095
exp(1.94882593 * (0.02265174 ^ 20) * (0.02274013 ^ 40) * -0.15270095)
exp(1.94882593) * exp(0.02265174 ^ 20) * exp(0.02274013 ^ 40) * exp(-0.15270095)
exp(1.94882593 * (0.02265174 ^ 20) * (0.02274013 ^ 40) * -0.15270095)
7.0204403 * (1.0229102 ^ 20) * (1.0230007 ^ 40) * 0.8583864
exp(predict(fit, ob))
7.0204403 * (1.0229102 ^ 20) * (1.0230007 ^ 40) * 0.8583864
ob <- data.frame(Base = c(20, 20), Age = c(30, 30), Trt = c("placebo", "progabide"))
exp(predict(fit, ob))
(21.84 - 18.75) / 21.84
library(tidyverse)
library(Rcpp)
library(dplyr)
update.packages()
library(DATA606)
library(tidyverse)
update.packages()
quit
quit()
library(tidyverse)
remove.packages("Rcpp")
install.packages("Rcpp")
library(tidyverse)
library(DATA606)
install.packages("digest")
library(DATA606)
library(dplyr)
remove.packages("dplyr")
install.packages("dplyr")
library(dplyr)
install.packages("dplyr")
library(dplyr)
install.packages("Rcpp",  dependencies = TRUE, repos = "http://cran.us.r-project.org")
install.packages("Rcpp", dependencies = TRUE, repos = "http://cran.us.r-project.org")
install.packages("yaml", dependencies = T)
library(dplyr)
install.packages("purrr")
library(purrr)
library(dplyr)
ls(pos = "package:dplyr")
library(dplyr)
library(DATA606)
install.packages("later")
library(DATA606)
library(Rcpp)
library(DATA606)
install.packages("later")
install.packages("later", dependencies = T)
install.packages("later", dependencies = T, TRUE)
install.packages("installr") # install
install.packages("installr") # install
installr::updateR(T) # updating R.
quit()
install.packages(c("bigrquery", "mailR"))
library(c("bigrquery", "mailR"))
library(bigrquery)
library(mailR)
## load packages
if(!require(bigrquery)){install.packages("bigrquery"); require(bigrquery)}
if(!require(rmarkdown)){install.packages("rmarkdown"); require(rmarkdown)}
if(!require(knitr)){install.packages("knitr"); require(knitr)}
if(!require(mailR)){install.packages("mailR"); require(mailR)}
if(!require(lubridate)){install.packages("lubridate"); require(lubridate)}
if(!require(dplyr)){install.packages("dplyr"); require(dplyr)}
if(!require(ggplot2)){install.packages("ggplot2"); require(ggplot2)}
if(!require(stringr)){install.packages("stringr"); require(stringr)}
if(!require(purrr)){install.packages("purrr"); require(purrr)}
if(!require(rlang)){install.packages("rlang"); require(rlang)}
# Big Query arguments
projectId <- "media-data-science"
dataset_staging <- "Jim_Story_Staging"
dataset_production <- "Jim_Story_Production"
dataset_audit <- "Jim_Story_Audit"
check_audit_a_query <- "
select max_date as last_entry_time
, sys_run_time as last_run_sys_time
, DATE_ADD(CURRENT_DATE(), 0, 'DAY') as current_run_time
, flag
from [media-data-science:Jim_Story_Audit.audit_a]
where job_number = (select max(job_number) from [media-data-science:Jim_Story_Audit.audit_a] where flag = 1)
"
# write query and save result
audit_a_maxdate <- query_exec(query = check_audit_a_query,
project = projectId,
use_legacy_sql = T)
# Big Query arguments
projectId <- "media-data-science"
dataset_staging <- "Jim_Story_Staging"
dataset_production <- "Jim_Story_Production"
dataset_audit <- "Jim_Story_Audit"
check_audit_a_query <- "
select max_date as last_entry_time
, sys_run_time as last_run_sys_time
, DATE_ADD(CURRENT_DATE(), 0, 'DAY') as current_run_time
, flag
from [media-data-science:Jim_Story_Audit.audit_a]
where job_number = (select max(job_number) from [media-data-science:Jim_Story_Audit.audit_a] where flag = 1)
"
# write query and save result
audit_a_maxdate <- query_exec(query = check_audit_a_query,
project = projectId,
use_legacy_sql = T)
ls(pos = "package:bigrquery")
red_flag2 <- query_exec(query = "select flag from [media-data-science:Jim_Story_Audit.audit_a]
where job_number = (select max(job_number) from [media-data-science:Jim_Story_Audit.audit_a])",
project = projectId,
use_legacy_sql = T) %>%
as.integer
set_access_cred(NULL)
get_access_cred()
library(purrr)
library(tidyr)
ls(pos = "package:purrr")
ls(pos = "package:tidyr")
code <- letters[1:12]
code <- expand.grid(code, code)
code
write.table(code, "clipboard", row.names = F)
?pmatch
things <- c("cat", "dog", "lion", "orange", "panther", "tiger")
fruit <- c("apple", "bananna", "orange" "pineapple")
things <- c("cat", "dog", "lion", "orange", "panther", "tiger")
pmatch(fruit, things)
fruit <- c("apple", "bananna", "orange" "pineapple")
fruit <- c("apple", "bananna", "orange", "pineapple")
things <- c("cat", "dog", "lion", "orange", "panther", "tiger")
pmatch(fruit, things)
pmatch(fruit, things, duplicates.ok = T)
pmatch(fruit, things, duplicates.ok = F)
fruit <- c("apple", "bananna", "orange", "pineapple", "orange")
pmatch(fruit, things, duplicates.ok = T)
pmatch(fruit, things, duplicates.ok = F)
pmatch(fruit, things, duplicates.ok = T)
?Encoding
?iconvlist
iconvlist()
install.packages("tau")
library(tau)
ls(pos = "package:tau")
?is.locale
setwd("C:/Users/traveler/Desktop/surge/surge_report_upload_code")
currentwd <- getwd()
library(readxl)
library(tidyverse)
library(rlang)
library(wrapr)
# set wd - pointing to where the xlsx files reside
setwd("staging")
# write a read() function
read <- function(x){
filename <- rlang::enquo(x)
filename <- stringr::str_sub(filename, 1,
stringr::str_length(filename) -9)
file <- paste0(filename, ".txt", sep = "")[2]
readxl::read_excel(x, sheet = "Bombora Comprehensive Report") %.>%
write.table(., file = file, row.names = F, sep = "\t")
}
# entity
setwd("entity")
entity_reports <- dir()
entity_reports
system.time( purrr::map(entity_reports, read) )
# theme
setwd("../"); setwd("theme")
theme_reports <- dir()
system.time( purrr::map(theme_reports, read) )
# reset wd
setwd(currentwd)
#################################################
### step 0 : load packages, set wd ##########
####################################
setwd("C:/Users/traveler/Desktop/job_posting_recommendation/code")
currentwd <- getwd()
if(!require(tidyverse)){install.packages("tidyverse"); require(tidyverse)}
if(!require(rvest)){install.packages("rvest"); require(rvest)}
if(!require(RCurl)){install.packages("RCurl"); require(RCurl)}
if(!require(plyr)){install.packages("plyr"); require(plyr)}
if(!require(wrapr)){install.packages("wrapr"); require(wrapr)}
if(!require(sqldf)){install.packages("sqldf"); require(sqldf)}
if(!require(Hmisc)){install.packages("Hmisc"); require(Hmisc)}
###########################################
### step 1 : get links ############
###############################
# identify the url(s) with the necessary parameters, such as job title, salary, distance, etc.
da <- "https://www.glassdoor.com/Job/jobs.htm?sc.keyword=data%20analyst&locT=C&locId=1132348&locKeyword=New%20York,%20NY&jobType=fulltime&fromAge=7&minSalary=100000&includeNoSalaryJobs=true&radius=-1&cityId=-1&minRating=0.0&industryId=-1&companyId=-1&applicationType=0&employerSizes=0&remoteWorkType=0"
bi <- "https://www.glassdoor.com/Job/jobs.htm?sc.keyword=business%20intelligence&locT=C&locId=1132348&locKeyword=New%20York,%20NY&jobType=fulltime&fromAge=7&minSalary=100000&includeNoSalaryJobs=true&radius=-1&cityId=-1&minRating=0.0&industryId=-1&companyId=-1&applicationType=0&employerSizes=0&remoteWorkType=0"
# get the job links and jobListingId from the above urls
da_links <- read_html(da) %>%
# html_nodes(".compactStars , .small , .empLoc div , .jobLink") %>%
html_nodes(".jobLink") %>%
html_attr("href") %>%
na.omit %>%
as.data.frame %>%
dplyr::rename("links" = ".") %>%
dplyr::mutate( links = paste0("https://www.glassdoor.com", as.character(links)),
jobListingId = stringr::str_extract_all(links, pattern = "jobListingId=[0-9]+") %.>%
gsub(pattern = "jobListingId=", replacement = "", x = .) ) %>%
arrange(., jobListingId, desc(links)) %>%
# many of these contain the same identical job posting but with different urls, we need to take them out
group_by(jobListingId) %>%
dplyr::mutate( id = row_number() ) %>%
ungroup %>%
dplyr::filter(id == 1) %>%
select(links, jobListingId)
bi_links <- read_html(bi) %>%
# html_nodes(".compactStars , .small , .empLoc div , .jobLink") %>%
html_nodes(".jobLink") %>%
html_attr("href") %>%
na.omit %>%
as.data.frame %>%
dplyr::rename("links" = ".") %>%
dplyr::mutate( links = paste0("https://www.glassdoor.com", as.character(links)),
jobListingId = stringr::str_extract_all(links, pattern = "jobListingId=[0-9]+") %.>%
gsub(pattern = "jobListingId=", replacement = "", x = .) ) %>%
arrange(., jobListingId, desc(links)) %>%
# many of these contain the same identical job posting but with different urls, we need to take them out
group_by(jobListingId) %>%
dplyr::mutate( id = row_number() ) %>%
ungroup %>%
dplyr::filter(id == 1) %>%
select(links, jobListingId)
# append the df
links <- rbind(da_links, bi_links) %>% distinct
# take a look of these links
# links %>%
#         arrange(desc(jobListingId)) %.>%
#         write.table(., "clipboard", row.names = F)
###################################################
### step 2 : get job info for each link #######
###########################################
# create extract function
extract_job_info <- function(link, jobListingId) {
read_html(link) %>%
html_nodes(".nowrap, .ratingNum , .salEst , .desc , #HeroHeaderModule .strong") %>%
html_text %>%
as.data.frame %>%
dplyr::rename("job" = ".") %>%
t %>%
as.data.frame %>%
dplyr::mutate(jobListingId = jobListingId)
}
# must use purrr::possibly - fail-safe approach
extract_job_info <- purrr::possibly(extract_job_info, otherwise = NA_real_)
# create an empty list
jobList <- vector(mode = "list", length = nrow(links))
# scrape the job info and write each into the list before complying into a data.frame
system.time( lapply(1:length(jobList), function(x){
jobList[[x]] <<- extract_job_info(links$links[x], links$jobListingId[x])
} ) %.>% invisible(.) )
# remove NA result first
na <- lapply(1:length(jobList), function(x) is.na(jobList[x])) %>% unlist %>% which
stay <- c(1:length(jobList))[1:length(jobList) %nin% na]
jobList <- jobList[stay]
# there's missing data in different job postings
# some have 4 or 5 columns, while most have 8 (complete set)
jobListSummary <- data.frame(row = 1:length(jobList), length = NA)
lapply(1:length(jobList), function(x) {
jobListSummary$length[x] <<- length(jobList[[x]])
}) %.>% invisible(.)
uniqueLength <- unique(jobListSummary$length)
lengthUniqueLength <- length(uniqueLength)
uniqueList <- vector(mode = "list", length = lengthUniqueLength)
for(i in 1:lengthUniqueLength){
uniqueList[[i]] <- sqldf::sqldf(
sprintf('select row from jobListSummary where length = %d', uniqueLength[i])
)
}
uniqueList <- lapply(1:length(uniqueList), function(x){
unlist(uniqueList[[x]]) %>% as.vector
})
# store each set of data into a data.frame by the length of column
jobListDf <- vector(mode = "list", length = lengthUniqueLength)
for(i in 1:lengthUniqueLength){
jobListDf[[i]] <- jobList[c(uniqueList[[i]])]
}
# jobListDf is now a list of lists (by length of column) in which each list contains a data.frame for each job
# str(jobListDf); summary(jobListDf)
# let's reduce it to a number of list by length of column and make each list a data.frame (combining all jobs inside that list)
putTogether <- function(x){
plyr::ldply(x, data.frame)
}
jobListDf <- lapply(jobListDf, putTogether)
# str(jobListDf)
#########################################################################
### step 3 : clean up each column, inner join back with "links" #######
###############################################################
### CAUTION : there are multiple lists - some have 9 columns, whereas some have only 5 or 6 ###
# let's just look at the complete set (9 columns)
# because the other are missing critical information that we need, e.g. review, salary
extractListNumber <- lapply(1:length(jobListDf), function(x){
length(jobListDf[[x]]) == 9}) %>%
unlist %.>%
which(. == T)
jobDf <- jobListDf[[extractListNumber]] %>%
dplyr::mutate( title = as.character(V1),
company = as.character(V2) %>%
stringr::str_trim(.),
last_updated = stringr::str_extract_all(V3, pattern = "Today|[0-9]+") %>%
unlist %.>%
ifelse(. == "Today", 0, .) %>%
as.numeric %.>%
lapply(1:length(.), function(x){
Sys.Date() -.[x]
}) %>%
unlist %.>%
as.Date(., origin = "1970-01-01"),
glassdoor_est = as.character(V4),
review = stringr::str_extract_all(V6, pattern = "[0-9]\\.[0-9]") %>%
as.numeric,
salary_est = gsub(pattern = ",", replacement = "", V7) %>%
stringr::str_extract_all(., pattern = "[0-9]+") %>%
unlist %>%
as.numeric,
salary_est = salary_est / 1000,
jobListingId = as.character(jobListingId),
description = as.character(V8),
run_time = Sys.time() ) %>%
tidyr::separate(., glassdoor_est, into = c("salary_min_est", "salary_max_est"), sep = "-") %>%
mutate( salary_min_est = stringr::str_extract_all(salary_min_est, pattern = "[0-9]+") %>%
unlist %>%
as.numeric,
salary_max_est = stringr::str_extract_all(salary_max_est, pattern = "[0-9]+") %>%
unlist %>%
as.numeric ) %>%
# inner join it back with "links" to get url pairing with jobListingId
dplyr::inner_join(., links, by = "jobListingId") %>%
dplyr::rename(url = links) %>%
select(jobListingId, title, company, last_updated,
salary_min_est, salary_max_est, salary_est,
review, url, run_time, description) %>%
arrange(company, title, last_updated)
# setwd to data/job
setwd("../"); setwd("data/job")
# save as txt
run_time <- jobDf$run_time %>% unique %.>% gsub(pattern = ":", replacement = "", x = .)
filename <- paste0("jobDf_", run_time, ".txt", sep = "")
write.table(jobDf, file = filename, sep = "\t", row.names = F, append = F)
# reset wd
setwd(currentwd)
###################################################
### step 4 : get, add company overview #######
###########################################
# write a function to search via google for the url pointing to the glassdoor overview database for a company
# wish I can just get an API to query against glassdoor/Indeed database
overview <- function(company){
entity = company
search = "glassdoor company overview"
url = URLencode(paste0("https://www.google.com/search?q=", paste0(entity, search, sep = " ")))
# return top pages result
cite <- read_html(url) %>%
html_nodes("cite") %>%  # change any node you like, e.g. cite
html_text() %>%
as.data.frame(., stringsAsFactors = F) %>%
dplyr::mutate(row = row_number()) %>%
dplyr::rename(url = ".")
# sql hack - return only the "lower(url) like '%glassdoor%overview/working-at%'"
overview_cite_select <- sqldf("select url
from (
select url, min(row) as flag
from ( select url, row
from cite
where lower(url) like '%glassdoor%overview/working-at%' ) x
group by url
) y") %>% as.character
# scrape the company overview
overview <- read_html(overview_cite_select) %>%
html_nodes(".value , .website") %>%
html_text %>%
# dplyr::rename("overview" = ".") %>%
t %>%
as.data.frame %>%
dplyr::mutate(V0 = company) %>%
select(., starts_with("V"))
return(overview)
}
# must use purrr::possibly - fail-safe approach
overview <- purrr::possibly(overview, otherwise = NA_real_)
# start scraping from the list of companies from jobDf
companies <- vector(mode = "list", length = length(unique(jobDf$company)))
names(companies) <- jobDf$company %>% unique
companies <- lapply(1:length(companies), function(x) companies[[x]] <- names(companies)[x])
# system.time( companies <- lapply(1:length(companies), function(x) overview(companies[[x]])) )
system.time( companies <- purrr::map(1:length(companies), function(x) overview(companies[[x]])) )  # a little faster
# remove NA result first
remove <- lapply(1:length(companies), function(x) is.na(companies[x])) %>% unlist %>% which
keep <- c(1:length(companies))[1:length(companies) %nin% remove]
# extract only V0:V7 columns
extract <- function(x){
x <- dplyr::select(x, V0, V1, V2, V3, V4, V5, V6, V7)
return(x)
}
# must use purrr::safely - fail-safe approach
extract_safely <- purrr::safely(extract, otherwise = NA_real_)
# extract, combine into a single df, and then rename columns
companyDf <- purrr::map(companies[keep], extract_safely) %.>%
purrr::transpose(.)
companyDf <- companyDf$result %>%
plyr::ldply(., data.frame)
names(companyDf) <- c("company", "website", "headquarters", "size",
"founded", "type", "industry", "revenue")
# change Factor to Chr, use str_trim to remove white space
companyDf <- companyDf[, 1:8]  # have to subset first; dplyr::select does not allow NA column
companyDf <- companyDf[complete.cases(companyDf), ] %>%
dplyr::mutate( website = as.character(website) %>% stringr::str_trim(.),
headquarters = as.character(headquarters) %>% stringr::str_trim(.),
size = as.character(size) %>% stringr::str_trim(.),
founded = stringr::str_extract_all(founded, "[0-9]+") %>% as.numeric,
type = as.character(type) %.>%
gsub(".*[Pp]rivate.*", "private", .) %.>%
gsub(".*[Pp]ublic.*", "public", .) %>%
stringr::str_trim(.),
industry = as.character(industry) %>% stringr::str_trim(.),
revenue = as.character(revenue) %>% stringr::str_trim(.),
run_time = Sys.time() )
# complete.cases() again, b/c of some misplacing columns
# some jobs have 10 columns and there's some mismatch when we manually pull V1:V7 columns
# for example, "founded" value would be misplaced in the "type" column
# an imperfect and temporary solution is to do complete.cases() one more time to rid the NA
companyDf <- companyDf[complete.cases(companyDf), ]
# setwd to data/company
setwd("../"); setwd("data/company")
# save as txt
run_time2 <- companyDf$run_time %>% unique %.>% gsub(pattern = ":", replacement = "", x = .)
filename2 <- paste0("companyDf_", run_time2, ".txt", sep = "")
write.table(companyDf, file = filename2, sep = "\t", row.names = F, append = F)
# reset wd
setwd(currentwd)
##############################################
##### step 5.1 : update (delete), insert #####
##############################################
setwd("../"); setwd("data")
# load the _agg tables
jobDf_agg <- readRDS(file = "jobDf_agg.RDS")
companyDf_agg <- readRDS(file = "companyDf_agg.RDS")
#######################
# delete, insert, save
# job
jobListingId_agg <- jobDf_agg$jobListingId
jobListingId_now <- jobDf$jobListingId
del <- jobListingId_agg[jobListingId_agg %in% jobListingId_now]
jobDf_agg <- jobDf_agg %>%
dplyr::filter(jobListingId %nin% del) %.>%  # delete
dplyr::bind_rows(., jobDf)  # append
saveRDS(jobDf_agg, file = "jobDf_agg.RDS")  # save
# company
company_agg <- companyDf_agg$company
company_now <- companyDf$company
del2 <- company_agg[company_agg %in% company_now]
companyDf_agg <- companyDf_agg %>%
dplyr::filter(company %nin% del2) %.>%  # delete
dplyr::bind_rows(., companyDf)  # append
saveRDS(companyDf_agg, file = "companyDf_agg.RDS")  # save
#######################
# reset wd
setwd(currentwd)
############################
##### step 5.2 : merge #####
############################
### CAUTION : there is always missing data for some companies overview ###
# let's merge (instead of left join) jobDf_agg and companyDf_agg together
jpr <- merge(jobDf_agg, companyDf_agg, by = "company") %>%
# filter job titles that are not relevant
dplyr::filter(title %in% title[!grepl("(architect|software|engineer|director)", title, ignore.case = T)]) %>%
# filter by job posting last_updated within past 14 days
# dplyr::filter(last_updated > Sys.Date() -14) %>%
# filter out duplicated jobListingId - this happens b/c same job with same Id can have different urls
group_by(jobListingId) %>%
dplyr::mutate(row = row_number()) %>%
ungroup %>%
dplyr::filter(row == 1) %>%
select(jobListingId, title, company, last_updated, salary_min_est, salary_max_est, salary_est, review, url,
website, headquarters, size, founded, type, industry, revenue, description) %>%
arrange(company, title, last_updated)
jpr %>% arrange(., desc(last_updated)) %>% View
quit()
